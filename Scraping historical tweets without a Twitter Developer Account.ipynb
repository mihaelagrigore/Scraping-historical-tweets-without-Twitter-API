{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping historical tweets without a Twitter Developer Account\n",
    "\n",
    "The tool we will use:\n",
    "- snscrape\n",
    "\n",
    "What you need: \n",
    "- Python 3.8\n",
    "\n",
    "What you don't need:\n",
    "- a Twitter Developer Account\n",
    "\n",
    "\n",
    "For a research project related to public discourse about results on international large scale assessments I needed to scrape historical tweets, going back all the way to the begining of Twitter. This is how I discovered **snscrape**, a wonderful tool, easy to setup and use. \n",
    "\n",
    "I didn't find snscrape from the start, initially I was reading through the intricate details of Twitter Developer Account, application procedure, different levels of access, limits etc etc. But luckily a friend recommended snscrape and suddenly the task of collecting tweets became extremely easy.\n",
    "\n",
    "Snscrape is a popular tool with social scientists for Tweets collection, at least in 2021. Apparently, it bypasses several limitations of the Twitter API.  \n",
    "The prettiest thing is that you don't need Twitter developer account credentials (like you do with <a href='https://www.tweepy.org/'>Tweepy</a>, for example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "\n",
    "1. [Installing snscrape](#1.-Installing-snscrape)\n",
    "2. [How to use snscrape](#2.-How-to-use-snscrape)\n",
    "3. [Calling snscrape CLI commands from Python Notebook](#3.-Calling-snscrape-CLI-commands-from-Python-Notebook)\n",
    "4. [Using snscrape Python wrapper](#4.-Using-snscrape-Python-wrapper)\n",
    "5. [Tweets meta-information gathered with snscrape](#5.-Tweets-meta-information-gathered-with-snscrape) \n",
    "6. [Dataset manipulation: JSON, CSV and Pandas DataFrame](#6.-Dataset-manipulation:-JSON,-CSV-and-Pandas-DataFrame)\n",
    "7. [Basic exploration of our collected dataset of tweets](#7.-Basic-exploration-of-our-collected-dataset-of-tweets)\n",
    "8. [Bonus: Publishing your Jupyter Notebook on Medium](#8.-Bonus:-Publishing-your-Jupyter-Notebook-on-Medium)\n",
    "9. [What next ? Sentiment analysis](#9.-What-next-?-Sentiment-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with some standard library imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import uuid\n",
    "\n",
    "from IPython.display import display_javascript, display_html, display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, date, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing snscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snscrape is available from its <a href='https://github.com/JustAnotherArchivist/snscrape'>official github project repository</a>.\n",
    "\n",
    "Snscrape has two versions:\n",
    "- released version, which you can install by running this line in a commant line terminal: **pip3 install snscrape** (for a Windows machine)\n",
    "- **development version**, which is said to have richer functionality, so this is the one I'll be using.   \n",
    "I will use the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's check the current Python version, as snscrape documentation mentions **it requires Python 3.8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't see 3.8.x in your case, please upgrade your Python version before you continue this tutorial, otherwise you will **not be able to install snscrape**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the development version of snscrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to c:\\users\\temp\\appdata\\local\\temp\\pip-req-build-0047h652\n",
      "Requirement already satisfied (use --upgrade to upgrade): snscrape==0.3.5.dev98+gf64ce21 from git+https://github.com/JustAnotherArchivist/snscrape.git in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\temp\\appdata\\roaming\\python\\python38\\site-packages (from snscrape==0.3.5.dev98+gf64ce21) (2.25.1)\n",
      "Requirement already satisfied: lxml in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from snscrape==0.3.5.dev98+gf64ce21) (4.5.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from snscrape==0.3.5.dev98+gf64ce21) (4.9.1)\n",
      "Requirement already satisfied: pytz in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from snscrape==0.3.5.dev98+gf64ce21) (2020.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\temp\\appdata\\roaming\\python\\python38\\site-packages (from requests[socks]->snscrape==0.3.5.dev98+gf64ce21) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\temp\\appdata\\roaming\\python\\python38\\site-packages (from requests[socks]->snscrape==0.3.5.dev98+gf64ce21) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\temp\\appdata\\roaming\\python\\python38\\site-packages (from requests[socks]->snscrape==0.3.5.dev98+gf64ce21) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\temp\\appdata\\roaming\\python\\python38\\site-packages (from requests[socks]->snscrape==0.3.5.dev98+gf64ce21) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from requests[socks]->snscrape==0.3.5.dev98+gf64ce21) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from beautifulsoup4->snscrape==0.3.5.dev98+gf64ce21) (2.0.1)\n",
      "Building wheels for collected packages: snscrape\n",
      "  Building wheel for snscrape (setup.py): started\n",
      "  Building wheel for snscrape (setup.py): finished with status 'done'\n",
      "  Created wheel for snscrape: filename=snscrape-0.3.5.dev98+gf64ce21-py3-none-any.whl size=50801 sha256=3f93a36eae72f4482ca953a1556968c0929e9301c71e64d460992144c5fe3494\n",
      "  Stored in directory: C:\\Users\\Temp\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-yk57j11p\\wheels\\92\\42\\87\\33fa9b18f7a75d02643a9ca3743339aec9be28c6796267c7d8\n",
      "Successfully built snscrape\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/JustAnotherArchivist/snscrape.git 'C:\\Users\\Temp\\AppData\\Local\\Temp\\pip-req-build-0047h652'\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How to use snscrape\n",
    "\n",
    "- through its command line interface (CLI) in the command prompt terminal.\n",
    "- use Python to run the CLI commands from a Jupyter notebook, for example (if you don't want to use the terminal to run commands)\n",
    "- or use the official snscrape Python wrapper. The Python wrapper is not well documented, unfortunately.\n",
    "\n",
    "Parameters you can use:\n",
    "- --jsonl : get the data into jsonl format\n",
    "- --progress\n",
    "- --max-results : limit the number of tweets to collect\n",
    "- --with-entity : Include the entity (e.g. user, channel) as the first output item (default: False)\n",
    "- --since DATETIME : Only return results newer than DATETIME (default: None)\n",
    "- --progress : Report progress on stderr (default: False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: snscrape [-h] [--version] [-v] [--dump-locals] [--retry N] [-n N]\r\n",
      "                [-f FORMAT | --jsonl] [--with-entity] [--since DATETIME]\r\n",
      "                [--progress]\r\n",
      "                {telegram-channel,vkontakte-user,weibo-user,facebook-group,instagram-user,instagram-hashtag,instagram-location,reddit-user,reddit-subreddit,reddit-search,twitter-thread,twitter-search,facebook-user,facebook-community,twitter-user,twitter-hashtag,twitter-list-posts,twitter-profile}\r\n",
      "                ...\r\n",
      "\r\n",
      "positional arguments:\r\n",
      "  {telegram-channel,vkontakte-user,weibo-user,facebook-group,instagram-user,instagram-hashtag,instagram-location,reddit-user,reddit-subreddit,reddit-search,twitter-thread,twitter-search,facebook-user,facebook-community,twitter-user,twitter-hashtag,twitter-list-posts,twitter-profile}\r\n",
      "                        The scraper you want to use\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --version             show program's version number and exit\r\n",
      "  -v, --verbose, --verbosity\r\n",
      "                        Increase output verbosity (default: 0)\r\n",
      "  --dump-locals         Dump local variables on serious log messages (warnings\r\n",
      "                        or higher) (default: False)\r\n",
      "  --retry N, --retries N\r\n",
      "                        When the connection fails or the server returns an\r\n",
      "                        unexpected response, retry up to N times with an\r\n",
      "                        exponential backoff (default: 3)\r\n",
      "  -n N, --max-results N\r\n",
      "                        Only return the first N results (default: None)\r\n",
      "  -f FORMAT, --format FORMAT\r\n",
      "                        Output format (default: None)\r\n",
      "  --jsonl               Output JSONL (default: False)\r\n",
      "  --with-entity         Include the entity (e.g. user, channel) as the first\r\n",
      "                        output item (default: False)\r\n",
      "  --since DATETIME      Only return results newer than DATETIME (default:\r\n",
      "                        None)\r\n",
      "  --progress            Report progress on stderr (default: False)\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run the snscrape help to see what options / parameters we can use\n",
    "cmd = 'snscrape --help'\n",
    "\n",
    "#This is similar to running os.system(cmd), which would show the output of running the command in the Terminal\n",
    "#window from where I started my Jupyter Notebook (which is what I used to develop this code)\n",
    "#By using subprocees, I capture the commands's output into a variable, whose content I can then print here.\n",
    "output = subprocess.check_output(cmd, shell=True)\n",
    "                                 \n",
    "print(output.decode(\"utf-8\"))                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calling snscrape CLI commands from Python Notebook\n",
    "\n",
    "Notice I make use of a few snscrape parameters:  \n",
    "- --max-results, to limit the search\n",
    "- --jsonl, to have my results saved directly into a json file\n",
    "- --since yyyy-mm-dd, so collect tweets starting with this date\n",
    "- twitter-search will tell snscrape what the actual text to search is.  \n",
    "    Notice I use the 'until:yyyy-mm-dd'. This is a workaround for the fact that sncrape does not have support for an --until DATETIME parameters.  \n",
    "    So I'm using Twitter's search <strong>until</strong> feature. That is, I am using a feature already built-in in Twitter search.  \n",
    "    For more <strong>search operators</strong> that you can use and pass on to snscrape as part of the text to search for, see the <a href='https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators'>Twitter documentation on search operators</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_filename = 'pisa2018-query-tweets.json'\n",
    "\n",
    "#Using the OS library to call CLI commands in Python\n",
    "os.system(f'snscrape --max-results 5000 --jsonl --progress --since 2018-12-01 twitter-search \"#pisa2018 lang:fr until:2019-12-31\" > {json_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using snscrape Python wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = date(2016, 12, 5)\n",
    "start = start.strftime('%Y-%m-%d')\n",
    "\n",
    "stop = date(2016, 12, 14)\n",
    "stop = stop.strftime('%Y-%m-%d')\n",
    "\n",
    "keyword = 'pisa2018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 1000\n",
    "\n",
    "#We are going to write the data into a csv file\n",
    "filename = keyword + start + '-' + stop + '.csv'\n",
    "csvFile = open(filename, 'a', newline='', encoding='utf8')\n",
    "\n",
    "#We write to the csv file by using csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id','date','tweet'])\n",
    "\n",
    "#I will use the following Twitter search operators:\n",
    "# since - start date for Tweets collection \n",
    "# stop  - stop date for Tweets collection\n",
    "# -filter:links - not very clear what this does, from Twitter search operators documentation: https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators\n",
    "#                 but it looks like this will exclude tweets with links from the search results\n",
    "# -filter:replies - removes @reply tweets from search results\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(keyword + 'since:' +  start + ' until:' + \\\n",
    "                                                        stop + ' -filter:links -filter:replies').get_items()):\n",
    "    if i > maxTweets :\n",
    "        break\n",
    "    csvWriter.writerow([tweet.id, tweet.date, tweet.content])\n",
    "\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tweets meta-information gathered with snscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at all the information that is available for every single tweet scraped using snscrape.  \n",
    "\n",
    "For this code I am using one example file that I made precidely for this, which contains a single JSON object. If you want to use a JSON file created with the steps above, you need to make some changes before you can run json.loads on it, as explained in <a href='https://stackoverflow.com/questions/21058935/python-json-loads-shows-valueerror-extra-data'>this stackoverflow discussion</a>.\n",
    "\n",
    "The solution for pretty printing JSON data inside a Jupyter Notebook comes from <a href='https://gist.github.com/nerevar/a068ee373e22391ad3a1413b3e554fb5'>this github project</a>.\n",
    "\n",
    "Click on the + icons to expand the contents of that particular item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"12f4acfe-bb6d-4c7e-a995-8944532ddb04\" style=\"height: 600px; width:100%;font: 12px/18px monospace !important;\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        require([\"https://rawgit.com/caldwell/renderjson/master/renderjson.js\"], function() {\n",
       "            renderjson.set_show_to_level(2);\n",
       "            document.getElementById('12f4acfe-bb6d-4c7e-a995-8944532ddb04').appendChild(renderjson([{\"url\": \"https://twitter.com/hlmdmrl/status/1144603007771955200\", \"date\": \"2019-06-28T13:46:37+00:00\", \"content\": \"#PISA2003 #PISA2006 \\n#PISA2009 #PISA2012 \\n#PISA2015 #egtkonus\\nT\\u00fcrkiye sonu\\u00e7lar\\u0131n\\u0131n ortak \\u00f6zelli\\u011fi:\\n1. Do\\u011fu Anadolu, G\\u00fcneydo\\u011fu Anadolu, Do\\u011fu Karadeniz B\\u00f6lgesine e\\u011filmeliyiz.\\n2. Meslek liselerine ve s\\u0131navla \\u00f6\\u011frenci almayan liselere e\\u011filmeliyiz.\\n3. Erkek \\u00f6\\u011frencilere e\\u011filmeliyiz. https://t.co/sywZhOGUKG\", \"renderedContent\": \"#PISA2003 #PISA2006 \\n#PISA2009 #PISA2012 \\n#PISA2015 #egtkonus\\nT\\u00fcrkiye sonu\\u00e7lar\\u0131n\\u0131n ortak \\u00f6zelli\\u011fi:\\n1. Do\\u011fu Anadolu, G\\u00fcneydo\\u011fu Anadolu, Do\\u011fu Karadeniz B\\u00f6lgesine e\\u011filmeliyiz.\\n2. Meslek liselerine ve s\\u0131navla \\u00f6\\u011frenci almayan liselere e\\u011filmeliyiz.\\n3. Erkek \\u00f6\\u011frencilere e\\u011filmeliyiz. twitter.com/bekirgur/statu\\u2026\", \"id\": 1144603007771955200, \"user\": {\"username\": \"hlmdmrl\", \"displayname\": \"hilmi demiral (do\\u00e7. dr.)\", \"id\": 77734713, \"description\": \"#Edebiyat\\u00d6\\u011fretimi | ESOG\\u00dc #egt \\u264c\\ufe0f #TYD\\u00d6 #DilK\\u00fclt\\u00fcr #Konu\\u015fma #Yazma #\\u00c7ocukEdebiyat\\u0131\", \"rawDescription\": \"#Edebiyat\\u00d6\\u011fretimi | ESOG\\u00dc #egt \\u264c\\ufe0f #TYD\\u00d6 #DilK\\u00fclt\\u00fcr #Konu\\u015fma #Yazma #\\u00c7ocukEdebiyat\\u0131\", \"descriptionUrls\": [], \"verified\": false, \"created\": \"2009-09-27T13:25:19+00:00\", \"followersCount\": 1974, \"friendsCount\": 260, \"statusesCount\": 21153, \"favouritesCount\": 16835, \"listedCount\": 12, \"mediaCount\": 1920, \"location\": \"Odunpazar\\u0131, T\\u00fcrkiye\", \"protected\": false, \"linkUrl\": \"https://ogu.academia.edu/hilmidemiral\", \"linkTcourl\": \"https://t.co/lHzyglizPX\", \"profileImageUrl\": \"https://pbs.twimg.com/profile_images/1377007933511327752/ID9b8ZxV_normal.jpg\", \"profileBannerUrl\": \"https://pbs.twimg.com/profile_banners/77734713/1565373450\", \"url\": \"https://twitter.com/hlmdmrl\"}, \"outlinks\": [\"https://twitter.com/bekirgur/status/1144598457992306688\"], \"tcooutlinks\": [\"https://t.co/sywZhOGUKG\"], \"replyCount\": 0, \"retweetCount\": 2, \"likeCount\": 3, \"quoteCount\": 0, \"conversationId\": 1144603007771955200, \"lang\": \"tr\", \"source\": \"<a href=\\\"http://twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web Client</a>\", \"sourceUrl\": \"http://twitter.com\", \"sourceLabel\": \"Twitter Web Client\", \"media\": null, \"retweetedTweet\": null, \"quotedTweet\": {\"url\": \"https://twitter.com/bekirgur/status/1144598457992306688\", \"date\": \"2019-06-28T13:28:32+00:00\", \"content\": \"LGS\\u2019de b\\u00f6lgeler aras\\u0131nda \\u00e7ok b\\u00fcy\\u00fck ba\\u015far\\u0131 farklar\\u0131 var. \\u2066@muberragormez\\u2069\\u2019\\u0131n analizi: https://t.co/rQ7JuJF1Al https://t.co/iaqJRlWXxc\", \"renderedContent\": \"LGS\\u2019de b\\u00f6lgeler aras\\u0131nda \\u00e7ok b\\u00fcy\\u00fck ba\\u015far\\u0131 farklar\\u0131 var. \\u2066@muberragormez\\u2069\\u2019\\u0131n analizi: setav.org/perspektif-201\\u2026 https://t.co/iaqJRlWXxc\", \"id\": 1144598457992306688, \"user\": {\"username\": \"bekirgur\", \"displayname\": \"Bekir G\\u00fcr\", \"id\": 4819139739, \"description\": \"Education Policy, Higher Education, Comparative and International Education. Academic/Akademisyen @ Ankara Y\\u0131ld\\u0131r\\u0131m Beyaz\\u0131t University.\", \"rawDescription\": \"Education Policy, Higher Education, Comparative and International Education. Academic/Akademisyen @ Ankara Y\\u0131ld\\u0131r\\u0131m Beyaz\\u0131t University.\", \"descriptionUrls\": [], \"verified\": false, \"created\": \"2016-01-16T20:50:47+00:00\", \"followersCount\": 7574, \"friendsCount\": 1241, \"statusesCount\": 6087, \"favouritesCount\": 3799, \"listedCount\": 25, \"mediaCount\": 440, \"location\": \"Ankara, Turkey\", \"protected\": false, \"linkUrl\": \"https://bekirgur.net\", \"linkTcourl\": \"https://t.co/QysZDoukXj\", \"profileImageUrl\": \"https://pbs.twimg.com/profile_images/731317683430510592/4ugnPcn2_normal.jpg\", \"profileBannerUrl\": \"https://pbs.twimg.com/profile_banners/4819139739/1595426062\", \"url\": \"https://twitter.com/bekirgur\"}, \"outlinks\": [\"https://www.setav.org/perspektif-2019-lgs-sonuclari-neler-soyluyor/\"], \"tcooutlinks\": [\"https://t.co/rQ7JuJF1Al\"], \"replyCount\": 0, \"retweetCount\": 5, \"likeCount\": 20, \"quoteCount\": 1, \"conversationId\": 1144598457992306688, \"lang\": \"tr\", \"source\": \"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone</a>\", \"sourceUrl\": \"http://twitter.com/download/iphone\", \"sourceLabel\": \"Twitter for iPhone\", \"media\": [{\"previewUrl\": \"https://pbs.twimg.com/media/D-JuOSPXUAEg_wA?format=jpg&name=small\", \"fullUrl\": \"https://pbs.twimg.com/media/D-JuOSPXUAEg_wA?format=jpg&name=large\", \"type\": \"photo\"}], \"retweetedTweet\": null, \"quotedTweet\": null, \"mentionedUsers\": [{\"username\": \"muberragormez\", \"displayname\": \"M\\u00fcberra G\\u00f6rmez Emin\", \"id\": 264258417, \"description\": null, \"rawDescription\": null, \"descriptionUrls\": null, \"verified\": null, \"created\": null, \"followersCount\": null, \"friendsCount\": null, \"statusesCount\": null, \"favouritesCount\": null, \"listedCount\": null, \"mediaCount\": null, \"location\": null, \"protected\": null, \"linkUrl\": null, \"linkTcourl\": null, \"profileImageUrl\": null, \"profileBannerUrl\": null, \"url\": \"https://twitter.com/muberragormez\"}]}, \"mentionedUsers\": null}]))\n",
       "        });\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = 'example.json'\n",
    "  \n",
    "with open(filename) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "class RenderJSON(object):\n",
    "    def __init__(self, json_data):\n",
    "        if isinstance(json_data, dict) or isinstance(json_data, list):\n",
    "            self.json_str = json.dumps(json_data)\n",
    "        else:\n",
    "            self.json_str = json_data\n",
    "        self.uuid = str(uuid.uuid4())\n",
    "\n",
    "    def _ipython_display_(self):\n",
    "        display_html('<div id=\"{}\" style=\"height: 600px; width:100%;font: 12px/18px monospace !important;\"></div>'.format(self.uuid), raw=True)\n",
    "        display_javascript(\"\"\"\n",
    "        require([\"https://rawgit.com/caldwell/renderjson/master/renderjson.js\"], function() {\n",
    "            renderjson.set_show_to_level(2);\n",
    "            document.getElementById('%s').appendChild(renderjson(%s))\n",
    "        });\n",
    "      \"\"\" % (self.uuid, self.json_str), raw=True)\n",
    "\n",
    "RenderJSON([data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset manipulation: JSON, CSV and Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting JSON to Pandas DataFrame\n",
    "\n",
    "Pandas DataFrame is **the** data structure of choice in Data Science, so we read the JSON file into a DataFrame.  \n",
    "\n",
    "Then we save it as CSV, since CSV is the most common file type for Data Science small projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pisa2018-query-tweets'\n",
    "tweets_df = pd.read_json(filename +'.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(327, 23)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>outlinks</th>\n",
       "      <th>tcooutlinks</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>...</th>\n",
       "      <th>lang</th>\n",
       "      <th>source</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>sourceLabel</th>\n",
       "      <th>media</th>\n",
       "      <th>retweetedTweet</th>\n",
       "      <th>quotedTweet</th>\n",
       "      <th>mentionedUsers</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://twitter.com/Netcole_fr/status/12102181...</td>\n",
       "      <td>2019-12-26 15:17:35+00:00</td>\n",
       "      <td>Le p√©riscolaire pour apprendre en s'amusant .....</td>\n",
       "      <td>Le p√©riscolaire pour apprendre en s'amusant .....</td>\n",
       "      <td>1210218104510386177</td>\n",
       "      <td>{'username': 'Netcole_fr', 'displayname': 'Net...</td>\n",
       "      <td>[https://www.amazon.fr/dp/1686530544]</td>\n",
       "      <td>[https://t.co/cC28XiWfc7]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>fr</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>https://mobile.twitter.com</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://twitter.com/HugoLamanna/status/1208736...</td>\n",
       "      <td>2019-12-22 13:08:48+00:00</td>\n",
       "      <td>La #Chine premi√®re du classement #PISA2018, pa...</td>\n",
       "      <td>La #Chine premi√®re du classement #PISA2018, pa...</td>\n",
       "      <td>1208736143585554432</td>\n",
       "      <td>{'username': 'HugoLamanna', 'displayname': 'La...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>fr</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://twitter.com/OSC_SciencesPo/status/1207...</td>\n",
       "      <td>2019-12-18 16:30:45+00:00</td>\n",
       "      <td>Le syst√®me scolaire fran√ßais au prisme des com...</td>\n",
       "      <td>Le syst√®me scolaire fran√ßais au prisme des com...</td>\n",
       "      <td>1207337415486136323</td>\n",
       "      <td>{'username': 'OSC_SciencesPo', 'displayname': ...</td>\n",
       "      <td>[https://www.sciencespo.fr/liepp/fr/content/le...</td>\n",
       "      <td>[https://t.co/l93v8cALQh]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>fr</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>https://mobile.twitter.com</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'username': 'LIEPP_ScPo', 'displayname': 'LI...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://twitter.com/Netcole_fr/status/12102181...   \n",
       "1  https://twitter.com/HugoLamanna/status/1208736...   \n",
       "2  https://twitter.com/OSC_SciencesPo/status/1207...   \n",
       "\n",
       "                       date  \\\n",
       "0 2019-12-26 15:17:35+00:00   \n",
       "1 2019-12-22 13:08:48+00:00   \n",
       "2 2019-12-18 16:30:45+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  Le p√©riscolaire pour apprendre en s'amusant .....   \n",
       "1  La #Chine premi√®re du classement #PISA2018, pa...   \n",
       "2  Le syst√®me scolaire fran√ßais au prisme des com...   \n",
       "\n",
       "                                     renderedContent                   id  \\\n",
       "0  Le p√©riscolaire pour apprendre en s'amusant .....  1210218104510386177   \n",
       "1  La #Chine premi√®re du classement #PISA2018, pa...  1208736143585554432   \n",
       "2  Le syst√®me scolaire fran√ßais au prisme des com...  1207337415486136323   \n",
       "\n",
       "                                                user  \\\n",
       "0  {'username': 'Netcole_fr', 'displayname': 'Net...   \n",
       "1  {'username': 'HugoLamanna', 'displayname': 'La...   \n",
       "2  {'username': 'OSC_SciencesPo', 'displayname': ...   \n",
       "\n",
       "                                            outlinks  \\\n",
       "0              [https://www.amazon.fr/dp/1686530544]   \n",
       "1                                                 []   \n",
       "2  [https://www.sciencespo.fr/liepp/fr/content/le...   \n",
       "\n",
       "                 tcooutlinks  replyCount  retweetCount  ...  lang  \\\n",
       "0  [https://t.co/cC28XiWfc7]           0             0  ...    fr   \n",
       "1                         []           1             0  ...    fr   \n",
       "2  [https://t.co/l93v8cALQh]           0             1  ...    fr   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "1  <a href=\"http://twitter.com/download/android\" ...   \n",
       "2  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "\n",
       "                             sourceUrl          sourceLabel media  \\\n",
       "0           https://mobile.twitter.com      Twitter Web App  None   \n",
       "1  http://twitter.com/download/android  Twitter for Android  None   \n",
       "2           https://mobile.twitter.com      Twitter Web App  None   \n",
       "\n",
       "  retweetedTweet quotedTweet  \\\n",
       "0            NaN        None   \n",
       "1            NaN        None   \n",
       "2            NaN        None   \n",
       "\n",
       "                                      mentionedUsers  coordinates place  \n",
       "0                                               None         None  None  \n",
       "1                                               None         None  None  \n",
       "2  [{'username': 'LIEPP_ScPo', 'displayname': 'LI...         None  None  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving DataFrame to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv(filename +'.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Basic exploration of our collected dataset of tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic introduction to tweets\n",
    "\n",
    "Tweets are 280 character messages (hence the name 'microblogging'). Just like on other social media platforms, you need to create an account and then you can start participating to the tweetverse.  \n",
    "\n",
    "Tweets act as short status updates. Tweets appear on timelines. Timelines are collections of tweets sorted in a chronological order. On your account's home page, you're shown a timeline where tweets from people you follow will be displayed. \n",
    "\n",
    "You can post your own brand new tweet, retweet an already existing tweet (which means ou just share the exact same tweet) or quote an existing tweet (similar to retweeting, but you can add your own comment to it). \n",
    "\n",
    "You can also reply to someone else's tweets or 'like' them.  \n",
    "\n",
    "Tweets often contain **entities**, which are mentions of:\n",
    "- other users, which appear in the form of @other_user\n",
    "- places\n",
    "- urls\n",
    "- media that was attached to the tweet\n",
    "- hashtags, that look like #example_hashtag. Hashtags are just a way to apply a label on a tweet. If I'm tweeting something about results of PISA, the Programme for International Student Assessment, I will likely use #oecdpisa in my tweet, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the number of Tweets we scraped  \n",
    "\n",
    "The following cell is overkill in this particular scenario, but imagine you just scraped 1 million tweets and you want to know how many you got. The cell below is a very efficient way to count in that case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327\n"
     ]
    }
   ],
   "source": [
    "num = sum(1 for line in open(json_filename))\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check tweets for a particular text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet number 1: üíº#PISA2018 : la France de justesse au dessus de la moyenne des pays de l'@OCDE_fr, des #in√©galit√©s sociales toujours importantes, il est relev√© chez les √©l√®ves la critique des conditions de la scolarit√©, l'autocensure...\n",
      "https://t.co/HafH4Kcnn5\n",
      "Lire ici : https://t.co/1CQq2yPE7y\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "substring = 'justesse'\n",
    "\n",
    "count = 0\n",
    "f = open(json_filename, 'r')\n",
    "for i, line in enumerate(f):\n",
    "    if substring in line:\n",
    "        count = count + 1\n",
    "        obj = json.loads(line)\n",
    "        print(f'Tweet number {count}: {obj[\"content\"]}')\n",
    "print(count)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual content of the tweet is available through test_df['content'] or test_df.content  \n",
    "\n",
    "renderedContent seems to contain the same information as content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Le p√©riscolaire pour apprendre en s'amusant ... #pisa2018\\n#UnPlanBpourL√©cole : https://t.co/cC28XiWfc7\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.iloc[0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links mentioned in the tweet are also listed separately in the outlinks column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.amazon.fr/dp/1686530544']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.iloc[0].outlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can gauge the popularity of a tweet through these features:\n",
    "- replyCount\n",
    "- retweetCount\n",
    "- likeCount\n",
    "- quoteCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "replyCount      0\n",
       "retweetCount    0\n",
       "likeCount       0\n",
       "quoteCount      0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popularity_columns = ['replyCount', 'retweetCount', 'likeCount', 'quoteCount']\n",
    "tweets_df.iloc[0][popularity_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most retweeted tweet in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content         #PISA2018\\nLa France m√©diocrement class√©e dans...\n",
       "retweetCount                                                  161\n",
       "Name: 103, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.iloc[tweets_df.retweetCount.idxmax()][['content','retweetCount']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bonus: Publishing your Jupyter Notebook on Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter_to_medium in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (0.2.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from jupyter_to_medium) (4.9.1)\n",
      "Requirement already satisfied: matplotlib>=3.1 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from jupyter_to_medium) (3.2.2)\n",
      "Requirement already satisfied: nbconvert in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from jupyter_to_medium) (5.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\temp\\appdata\\roaming\\python\\python38\\site-packages (from jupyter_to_medium) (1.19.5)\n",
      "Requirement already satisfied: requests in c:\\users\\temp\\appdata\\roaming\\python\\python38\\site-packages (from jupyter_to_medium) (2.25.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from beautifulsoup4->jupyter_to_medium) (2.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from matplotlib>=3.1->jupyter_to_medium) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from matplotlib>=3.1->jupyter_to_medium) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from matplotlib>=3.1->jupyter_to_medium) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from matplotlib>=3.1->jupyter_to_medium) (2.4.7)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from nbconvert->jupyter_to_medium) (0.3)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from nbconvert->jupyter_to_medium) (4.3.3)\n",
      "Requirement already satisfied: defusedxml in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from nbconvert->jupyter_to_medium) (0.6.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from nbconvert->jupyter_to_medium) (0.8.4)\n",
      "Requirement already satisfied: pygments in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from nbconvert->jupyter_to_medium) (2.6.1)\n",
      "Requirement already satisfied: nbformat>=4.4 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from nbconvert->jupyter_to_medium) (5.0.7)\n",
      "Requirement already satisfied: jupyter-core in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from nbconvert->jupyter_to_medium) (4.6.3)\n",
      "Requirement already satisfied: jinja2>=2.4 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from nbconvert->jupyter_to_medium) (2.11.2)\n",
      "Requirement already satisfied: bleach in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from nbconvert->jupyter_to_medium) (3.1.5)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from nbconvert->jupyter_to_medium) (1.4.2)\n",
      "Requirement already satisfied: testpath in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from nbconvert->jupyter_to_medium) (0.4.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\temp\\appdata\\roaming\\python\\python38\\site-packages (from requests->jupyter_to_medium) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\temp\\appdata\\roaming\\python\\python38\\site-packages (from requests->jupyter_to_medium) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\temp\\appdata\\roaming\\python\\python38\\site-packages (from requests->jupyter_to_medium) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\temp\\appdata\\roaming\\python\\python38\\site-packages (from requests->jupyter_to_medium) (1.26.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\temp\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.1->matplotlib>=3.1->jupyter_to_medium) (1.15.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from traitlets>=4.2->nbconvert->jupyter_to_medium) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from traitlets>=4.2->nbconvert->jupyter_to_medium) (4.4.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from nbformat>=4.4->nbconvert->jupyter_to_medium) (3.2.0)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from jupyter-core->nbconvert->jupyter_to_medium) (227)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from jinja2>=2.4->nbconvert->jupyter_to_medium) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from bleach->nbconvert->jupyter_to_medium) (20.4)\n",
      "Requirement already satisfied: webencodings in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from bleach->nbconvert->jupyter_to_medium) (0.5.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\temp\\appdata\\roaming\\python\\python38\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter_to_medium) (54.1.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter_to_medium) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\stuff\\anaconda3\\envs\\workspace\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter_to_medium) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jupyter_to_medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Stuff\\anaconda3\\envs\\workspace\\lib\\site-packages\\nbconvert\\filters\\datatypefilter.py:39: UserWarning: Your element with mimetype(s) dict_keys(['application/javascript']) is not able to be represented.\n",
      "  warn(\"Your element with mimetype(s) {mimetypes}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading image to medium\n",
      "\n",
      "\n",
      "Image Storage Information from Medium\n",
      "-------------------------------------\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"url\": \"https://cdn-images-1.medium.com/proxy/1*9fYa3dFxd2p345blmwjnuQ.png\",\n",
      "            \"md5\": \"9fYa3dFxd2p345blmwjnuQ\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "Successfully posted to Medium!!!\n",
      "--------------------------------\n",
      "id                  79a2c61f76ab\n",
      "title               Scraping historical tweets without a Twitter Developer Account\n",
      "authorId            10c72e180be4b304a8c49f0e230277f5e2b89dd743592deaa2d8bdc3886f216fe\n",
      "url                 https://medium.com/@mihaelagrigore/79a2c61f76ab\n",
      "canonicalUrl        \n",
      "publishStatus       draft\n",
      "license             all-rights-reserved\n",
      "licenseUrl          https://policy.medium.com/medium-terms-of-service-9db0094a1e0f\n",
      "tags                ['scraping-with-python', 'twitter-archive']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data': {'id': '79a2c61f76ab',\n",
       "  'title': 'Scraping historical tweets without a Twitter Developer Account',\n",
       "  'authorId': '10c72e180be4b304a8c49f0e230277f5e2b89dd743592deaa2d8bdc3886f216fe',\n",
       "  'url': 'https://medium.com/@mihaelagrigore/79a2c61f76ab',\n",
       "  'canonicalUrl': '',\n",
       "  'publishStatus': 'draft',\n",
       "  'license': 'all-rights-reserved',\n",
       "  'licenseUrl': 'https://policy.medium.com/medium-terms-of-service-9db0094a1e0f',\n",
       "  'tags': ['scraping-with-python', 'twitter-archive']}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jupyter_to_medium as jtm\n",
    "jtm.publish('Scraping historical tweets without a Twitter Developer Account.ipynb',\n",
    "            integration_token='2896a01c2565cfe9209fb96dab1bf2fb79614391b0f6abbc46aca1dc3fba9b7c1',\n",
    "            pub_name=None,\n",
    "            title='Scraping historical tweets without a Twitter Developer Account',\n",
    "            tags=['scraping with Python', 'Twitter archive'],\n",
    "            publish_status='draft',\n",
    "            notify_followers=False,\n",
    "            license='all-rights-reserved',\n",
    "            canonical_url=None,\n",
    "            chrome_path=None,\n",
    "            save_markdown=False,\n",
    "            table_conversion='chrome'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's about it for a quick intro to scraping tweets without the need to apply for a Twitter Developer Account and with no limitations for the maximum number of tweets we can get or for how far back in time we can go.\n",
    "\n",
    "## 9. What next ? Sentiment analysis\n",
    "\n",
    "What to do next with the tweets you just scraped ? In my case, I was very interested in <a href='https://www.kaggle.com/mishki/twitter-sentiment-analysis-using-nlp-techniques'>NLP for sentiment analysis of tweets</a>, or you may try topic modelling using Latent Dirichlet Allocation (LDA) or build a network graph from this data and use network analysis methods on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
